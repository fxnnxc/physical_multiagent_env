episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,timesteps_total,done,episodes_total,training_iteration,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,trial_id,hist_stats/episode_reward,hist_stats/episode_lengths,timers/sample_time_ms,timers/sample_throughput,timers/learn_time_ms,timers/learn_throughput,info/num_steps_sampled,info/num_steps_trained,perf/cpu_util_percent,perf/ram_util_percent,info/learner/pol/allreduce_latency,info/learner/pol/cur_kl_coeff,info/learner/pol/cur_lr,info/learner/pol/total_loss,info/learner/pol/policy_loss,info/learner/pol/vf_loss,info/learner/pol/vf_explained_var,info/learner/pol/kl,info/learner/pol/entropy,info/learner/pol/entropy_coeff
nan,nan,nan,nan,0,0,4000,False,0,1,84ec11b801b04cdd9d67954ecae714e8,2021-04-30_18-03-38,1619773418,40.7036349773407,40.7036349773407,44429,Hinton,143.248.157.211,40.7036349773407,0,1,d3a82_00000,[],[],23104.497,173.126,17598.315,227.294,4000,4000,17.40344827586207,47.19827586206896,0.0,0.2,5e-05,0.05203880101907998,-0.01387062028516084,0.06113137805368751,0.22230354,0.023890229174867272,1.5850418135523796,0.0
